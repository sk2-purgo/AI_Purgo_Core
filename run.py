from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import BertModel, AutoTokenizer
import torch.nn as nn
import os
import re
from typing import List
from asyncio import to_thread
import logging
from dotenv import load_dotenv
from openai import OpenAI

# âœ… ë¡œê·¸ ë ˆë²¨ ë‚®ì¶”ê¸° (transformers ë¶ˆí•„ìš”í•œ ê²½ê³  ìˆ¨ê¸°ê¸°)
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… KoBERT ëª¨ë¸ ì •ì˜
class CussClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained("monologg/kobert", trust_remote_code=True)
        self.classifier = nn.Linear(768, 2)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return self.classifier(outputs.pooler_output)

# âœ… KoBERT ëª¨ë¸ ë¡œë”©
model = CussClassifier().to(device)
model_path = os.path.join("purgo_kobert", "model", "purgo.pth")
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()

# âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë”©
kobert_tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)

# âœ… ìš•ì„¤ ë‹¨ì–´ ì‚¬ì „ ë¡œë”©
def load_badwords_from_txt():
    path = os.path.join("purgo_kobert", "app", "befasttext_filter", "befasttext_cuss_train_full.txt")
    badwords = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            word = line.strip().split(",")[0]
            if word:
                badwords.add(word)
    return badwords

badword_set = load_badwords_from_txt()  # âœ… ë°˜ë“œì‹œ í™œì„±í™”

# âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def clean_text(text):
    text = re.sub(r"[^\u4E00-\u9FFF\u3400-\u4DBFê°€-í£ã„±-ã…ã…-ã…£0-9\s]", "", text)
    return text

# âœ… FastText ê°ì§€
def detect_fasttext(text: str, badwords: set) -> List[str]:
    cleaned_text = clean_text(text)
    return [word for word in badwords if word in cleaned_text]

# âœ… KoBERT ê°ì§€
def detect_kobert(text):
    inputs = kobert_tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=64)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        probs = torch.softmax(outputs, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        conf = probs[0][pred].item()
    return pred, round(conf, 4)

# âœ… GPT-3.5 ìˆœí™”
def rewrite_text_gpt3_5(text):
    prompt = f"""
ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì˜ ë°œí™”ë¥¼ ì •ì¤‘í•˜ê³  ê¸ì •ì ì¸ í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•˜ëŠ” AIì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì¥ì„ ì•„ë˜ ì¡°ê±´ì— ë§ì¶° ìˆœí™”í•´ì£¼ì„¸ìš”:

[ğŸ’¬ ìˆœí™” ì¡°ê±´]
1. ë¬¸ì¥ì— ìš•ì„¤, ë¹„ì†ì–´, í˜ì˜¤, ì„±ì ì¸ í‘œí˜„ì´ ìˆì„ ê²½ìš° â†’ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ **ì •ì¤‘í•˜ê³  ë°”ë¥¸ í‘œí˜„**ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë°”ê¾¸ì„¸ìš”.
2. ë…¸ê³¨ì ì´ì§€ ì•Šì•„ë„ ê³µê²©ì ì´ê±°ë‚˜ ë¶€ì •ì ì¸ ë‰˜ì•™ìŠ¤ë¥¼ ê°€ì§„ ë‹¨ì–´ëŠ” **ê¸ì •ì ì´ê³  í¬ìš©ì ì¸ í‘œí˜„**ìœ¼ë¡œ ìˆœí™”í•˜ì„¸ìš”.
3. **ë¬¸ì¥ì˜ êµ¬ì¡°ì™€ ë§íˆ¬ëŠ” ìœ ì§€**í•˜ë©´ì„œ, ë¬¸ì œê°€ ë˜ëŠ” ë‹¨ì–´ë§Œ ë°”ê¾¸ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.
4. ì˜ˆì˜ ë°”ë¥´ê³  ë¶€ë“œëŸ¬ìš´ ì–´íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
5. **ìš•ì„¤ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” ë¬¸ì¥ì„ ìˆ˜ì •í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜**í•˜ì„¸ìš”.
6. ì¶œë ¥ì€ ë°˜ë“œì‹œ **ì •ì œëœ ë¬¸ì¥ í•œ ì¤„ë§Œ**, ì„¤ëª…ì´ë‚˜ ë”°ì˜´í‘œ ì—†ì´ ì¶œë ¥í•˜ì„¸ìš”.

[ğŸ”´ ë°˜ë“œì‹œ ìˆœí™”í•´ì•¼ í•  í‘œí˜„ ì˜ˆì‹œ]
- ì´ˆì„± ìš•ì„¤ (ì˜ˆ: ã……ã…‚, ã…ˆã„¹, ã…„ ë“±)
- ê°ì • ê³¼ê²© í‘œí˜„ (ì˜ˆ: ì¡´ë‚˜, ê°œê°™ì€, ì§€ë„ ë“±)
- ì¸ì‹  ê³µê²© í‘œí˜„ (ì˜ˆ: ë¯¸ì¹œë†ˆ, ë³‘ì‹ , ìƒˆë¼, ë¸…ì‹  ë“±)

[ğŸ§  ìˆœí™” ì˜ˆì‹œ]
- "ì”¨ë°œ ì˜¤ëŠ˜ ì™œ ì´ë˜" â†’ "ì•„ ì§„ì§œ ì˜¤ëŠ˜ ì™œ ì´ë˜"
- "ê°œê°™ì€ ìƒˆë¼" â†’ "ì •ë§ ëª»ëœ ì‚¬ëŒ"
- "ì¡´ë‚˜ ì§œì¦ë‚˜" â†’ "ì •ë§ ì§œì¦ë‚˜"
- "ì§€ë„í•˜ì§€ ë§ˆ" â†’ "í™”ë‚´ì§€ ë§ˆ"
- "ë³‘ì‹ ê°™ì´ í•˜ë„¤" â†’ "ì–´ìˆ˜ë£©í•˜ê²Œ í•˜ë„¤"
- "ã……ã…‚" â†’ "ì•„ ì§„ì§œ"
- "ì €ìƒˆë¼ ë­ì•¼" â†’ "ì € ì‚¬ëŒ ë­ì§€"

ë¬¸ì¥: "{text}"

"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì¥ì„ ì •ì œí•˜ëŠ” ìˆœí™” ì „ìš© í¸ì§‘ê¸°ì•¼. ì„¤ëª… ì—†ì´ ë°”ë¥¸ ë§ë¡œ ë°”ê¿”ì¤˜."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=100,
            top_p=0.9,
        )
        result = response.choices[0].message.content.strip()
        if result.startswith('"') and result.endswith('"'):
            result = result[1:-1]
        return result
    except Exception as e:
        print(f"âŒ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return text

# âœ… FastAPI ì„œë²„ ì‹œì‘
app = FastAPI()

class TextRequest(BaseModel):
    text: str

@app.get("/")
async def root():
    return {"message": "âœ… FastText + KoBERT + GPT-3.5 ë¹„ë™ê¸° ì„œë²„ ì‹¤í–‰ ì¤‘!"}

@app.post("/analyze")  # â— ì•ì— "/" í•„ìˆ˜
async def analyze(req: TextRequest):
    text = req.text.strip()
    fasttext_result = detect_fasttext(text, badword_set)
    fasttext_hit = 1 if fasttext_result else 0

    response = {
        "fasttext": {
            "is_bad": fasttext_hit,
            "detected_words": fasttext_result
        },
        "kobert": {
            "is_bad": None,
            "confidence": None
        },
        "result": {
            "original_text": text,
            "rewritten_text": text
        },
        "final_decision": 0
    }

    if fasttext_hit:
        rewritten = await to_thread(rewrite_text_gpt3_5, text)
        response["result"]["rewritten_text"] = rewritten  # ğŸ”§
        response["final_decision"] = 1
    else:
        kobert_pred, kobert_conf = await to_thread(detect_kobert, text)
        kobert_hit = 1 if kobert_pred == 1 else 0
        response["kobert"]["is_bad"] = kobert_hit
        response["kobert"]["confidence"] = kobert_conf
        if kobert_hit:
            rewritten = await to_thread(rewrite_text_gpt3_5, text)
            response["result"]["rewritten_text"] = rewritten
            response["final_decision"] = 1

    return response

# uvicorn gpt_be_run:app --host 0.0.0.0 --port 5000 --reload
